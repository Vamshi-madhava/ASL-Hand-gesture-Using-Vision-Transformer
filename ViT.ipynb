{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "from dataclasses import dataclass\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import math, os, zipfile, shutil, random\n",
        "from torchvision import transforms, datasets\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from tqdm import tqdm\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "\n",
        "\n",
        "zip_path = '/content/drive/MyDrive/archive.zip'\n",
        "extract_dir = '/content/asl_alphabet'\n",
        "\n",
        "if not os.path.exists(extract_dir):\n",
        "    print(\"Extracting archive.zip...\")\n",
        "    shutil.copy(zip_path, '/content/archive.zip')\n",
        "    try:\n",
        "        with zipfile.ZipFile('/content/archive.zip', 'r') as zip_ref:\n",
        "            zip_ref.extractall(extract_dir)\n",
        "        print(\"Extraction successful.\")\n",
        "    except zipfile.BadZipFile:\n",
        "        print(\"Error: Not a valid zip file.\")\n",
        "else:\n",
        "    print(\"Dataset already extracted.\")\n",
        "\n",
        "train_dir = os.path.join(extract_dir, 'asl_alphabet_train', 'asl_alphabet_train')\n",
        "image_size = (100, 100)\n",
        "\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(image_size),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "@dataclass\n",
        "class Config:\n",
        "  n_embd: int = 768\n",
        "  n_head: int = 12\n",
        "  n_layer: int = 12\n",
        "  n_class: int = 29\n",
        "  patch_size: int = 16\n",
        "  max_len: int = (100 // patch_size) * (100 // patch_size)  # 100x100 image\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class RMSNorm(nn.Module):\n",
        "    def __init__(self, dim, eps=1e-5):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.scale = nn.Parameter(torch.ones(dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        norm = torch.sqrt(x.pow(2).mean(dim=-1, keepdim=True) + self.eps)\n",
        "        return self.scale * (x / (norm + self.eps))\n",
        "\n",
        "class RotaryEmbedding(nn.Module):\n",
        "    def __init__(self, dim, base=10000):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n",
        "        self.register_buffer(\"inv_freq\", inv_freq)\n",
        "\n",
        "    def forward(self, seq_len, device=None):\n",
        "        # shape: (seq_len)\n",
        "        positions = torch.arange(seq_len, device=device).type_as(self.inv_freq)\n",
        "        # outer product → (seq_len, dim/2)\n",
        "        angles = torch.einsum(\"i,j->ij\", positions, self.inv_freq)\n",
        "        # (seq_len, dim/2, 2)\n",
        "        emb = torch.stack((torch.cos(angles), torch.sin(angles)), dim=-1)\n",
        "        return emb  # shape: (seq_len, dim/2, 2)\n",
        "\n",
        "def apply_rotary_emb(x, rope):\n",
        "    # x: (batch, heads, seq, dim)\n",
        "    # rope: (seq, dim/2, 2)\n",
        "    x1 = x[..., ::2]  # even dims\n",
        "    x2 = x[..., 1::2] # odd dims\n",
        "\n",
        "    cos = rope[..., 0].unsqueeze(0).unsqueeze(0)  # (1, 1, seq, dim/2)\n",
        "    sin = rope[..., 1].unsqueeze(0).unsqueeze(0)\n",
        "\n",
        "    x_rotated_even = x1 * cos - x2 * sin\n",
        "    x_rotated_odd = x1 * sin + x2 * cos\n",
        "\n",
        "    return torch.stack([x_rotated_even, x_rotated_odd], dim=-1).flatten(-2)\n",
        "\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_head == 0\n",
        "        self.rope = RotaryEmbedding(config.n_embd // config.n_head)\n",
        "\n",
        "\n",
        "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias = False)\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias = False)\n",
        "\n",
        "\n",
        "        self.n_head = config.n_head\n",
        "        self.n_embd = config.n_embd\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size()  # Batch, Time, Channels\n",
        "        qkv = self.c_attn(x)  # shape: (B, T, 3*C)\n",
        "\n",
        "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
        "\n",
        "        # Reshape: (B, heads, T, head_dim)\n",
        "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "\n",
        "        # Apply  (RoPE)\n",
        "        rope = self.rope(T, device=x.device)  # (T, head_dim/2, 2)\n",
        "        q = apply_rotary_emb(q, rope)  # shape unchanged\n",
        "        k = apply_rotary_emb(k, rope)\n",
        "\n",
        "        # Attention score calculation\n",
        "        att = (q @ k.transpose(-2, -1)) * (1.0 / (C // self.n_head) ** 0.5)\n",
        "        att = F.softmax(att, dim=-1)\n",
        "\n",
        "        # Apply attention to values\n",
        "        y = att @ v  # (B, heads, T, head_dim)\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C)  # reshape to (B, T, C)\n",
        "\n",
        "        # Final projection\n",
        "        y = self.c_proj(y)\n",
        "        return y\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd)\n",
        "        self.gelu = nn.GELU(approximate = 'tanh')\n",
        "        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd)\n",
        "    def forward(self, x):\n",
        "        x = self.c_fc(x)\n",
        "        x = self.gelu(x)\n",
        "        x = self.c_proj(x)\n",
        "        return x\n",
        "\n",
        "class PatchEmbedding(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "\n",
        "        self.proj = nn.Conv2d(\n",
        "            in_channels =  3,\n",
        "            out_channels = config.n_embd,\n",
        "            kernel_size = config.patch_size,\n",
        "            stride = config.patch_size\n",
        "        )\n",
        "    def forward(self,x):\n",
        "      x = self.proj(x)\n",
        "      x = x.flatten(2)\n",
        "      x = x.transpose(1,2)\n",
        "      return x\n",
        "\n",
        "class VisionTransformer(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.patch_embedding = PatchEmbedding(config)\n",
        "\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, config.n_embd))\n",
        "        # self.pos_embedding = nn.Parameter(torch.zeros(1, 1 + config.max_len, config.n_embd))\n",
        "\n",
        "        self.blocks = nn.ModuleList([Block(config) for _ in range(config.n_layer)])\n",
        "\n",
        "        self.head = nn.Linear(config.n_embd, config.n_class)\n",
        "\n",
        "        self.ln_f = RMSNorm(config.n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.patch_embedding(x)\n",
        "        cls_token = self.cls_token.expand(x.shape[0], -1, -1)\n",
        "        x = torch.cat((cls_token, x), dim=1)\n",
        "        # x = x + self.pos_embedding\n",
        "        for block in self.blocks:\n",
        "          x = block(x)\n",
        "        x = self.ln_f(x)\n",
        "        cls_output = x[:, 0]  # Shape: (B, C)\n",
        "        return self.head(cls_output)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "\n",
        "        self.ln1 = RMSNorm(config.n_embd)\n",
        "        self.attention = SelfAttention(config)\n",
        "        self.ln2 = RMSNorm(config.n_embd)\n",
        "        self.mlp = MLP(config)\n",
        "    def forward(self, x):\n",
        "        x = x + self.attention(self.ln1(x))\n",
        "        x = x + self.mlp(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "full_dataset = datasets.ImageFolder(train_dir, transform=transform)\n",
        "val_percent = 0.1\n",
        "val_size = int(val_percent * len(full_dataset))\n",
        "train_size = len(full_dataset) - val_size\n",
        "\n",
        "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size], generator=torch.Generator().manual_seed(42))\n",
        "train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True, num_workers=os.cpu_count(), pin_memory=True, prefetch_factor=32)\n",
        "val_loader = DataLoader(val_dataset, batch_size=512, shuffle=False, num_workers=os.cpu_count(), pin_memory=True)\n",
        "\n",
        "best_val_acc = 0.0\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "\n",
        "config = Config()\n",
        "model = VisionTransformer(config).to(device)\n",
        "model = torch.compile(model)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=0.01)\n",
        "scheduler = CosineAnnealingLR(optimizer, T_max=10, eta_min=1e-6)\n",
        "early_stopping_patience = 5\n",
        "early_stopping_counter = 0\n",
        "\n",
        "\n",
        "num_epochs = 10\n",
        "\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    train_loss, train_correct, total = 0.0, 0, 0\n",
        "\n",
        "    for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
        "        images, labels = images.to(device,memory_format = torch.channels_last, non_blocking = True), labels.to(device, non_blocking = True)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        with torch.cuda.amp.autocast():\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        train_correct += (outputs.argmax(1) == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "    train_acc = 100 * train_correct / total\n",
        "\n",
        "\n",
        "    model.eval()\n",
        "    val_correct, val_total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            with torch.cuda.amp.autocast():\n",
        "                outputs = model(images)\n",
        "            val_correct += (outputs.argmax(1) == labels).sum().item()\n",
        "            val_total += labels.size(0)\n",
        "\n",
        "    val_acc = 100 * val_correct / val_total\n",
        "\n",
        "    print(f\"\\nEpoch {epoch+1}: Train Loss = {train_loss/len(train_loader):.4f} | Train Acc = {train_acc:.2f}% | Val Acc = {val_acc:.2f}%\")\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        early_stopping_counter = 0\n",
        "        torch.save(model.state_dict(), \"/content/best_vit_model.pth\")\n",
        "    else:\n",
        "        early_stopping_counter+= 1\n",
        "        if early_stopping_counter >= early_stopping_patience:\n",
        "            print(f\"Early stopping triggered after {epoch+1} epochs.\")\n",
        "            break\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZDfUSS121OWA",
        "outputId": "d07c244d-7fd6-43c3-ef5b-f20c96119057"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Extracting archive.zip...\n",
            "Extraction successful.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1-1603501424.py:223: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler()\n",
            "Epoch 1/10:   0%|          | 0/153 [00:00<?, ?it/s]/tmp/ipython-input-1-1603501424.py:247: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n",
            "W0716 12:31:05.469000 309 torch/_inductor/utils.py:1137] [0/0] Not enough SMs to use max_autotune_gemm mode\n",
            "Epoch 1/10: 100%|██████████| 153/153 [03:21<00:00,  1.32s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1: Train Loss = 2.0615 | Train Acc = 35.79% | Val Acc = 75.17%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/10: 100%|██████████| 153/153 [00:59<00:00,  2.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 2: Train Loss = 0.4066 | Train Acc = 85.81% | Val Acc = 89.85%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/10: 100%|██████████| 153/153 [01:00<00:00,  2.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 3: Train Loss = 0.1639 | Train Acc = 94.39% | Val Acc = 95.60%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/10: 100%|██████████| 153/153 [01:01<00:00,  2.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 4: Train Loss = 0.0721 | Train Acc = 97.56% | Val Acc = 97.72%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/10: 100%|██████████| 153/153 [01:01<00:00,  2.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 5: Train Loss = 0.0275 | Train Acc = 99.16% | Val Acc = 98.98%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/10: 100%|██████████| 153/153 [01:01<00:00,  2.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 6: Train Loss = 0.0091 | Train Acc = 99.75% | Val Acc = 99.53%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/10: 100%|██████████| 153/153 [01:01<00:00,  2.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 7: Train Loss = 0.0030 | Train Acc = 99.93% | Val Acc = 99.82%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/10: 100%|██████████| 153/153 [01:01<00:00,  2.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 8: Train Loss = 0.0008 | Train Acc = 99.99% | Val Acc = 99.85%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/10: 100%|██████████| 153/153 [01:01<00:00,  2.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 9: Train Loss = 0.0005 | Train Acc = 100.00% | Val Acc = 99.83%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/10: 100%|██████████| 153/153 [01:01<00:00,  2.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 10: Train Loss = 0.0004 | Train Acc = 100.00% | Val Acc = 99.83%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), \"/content/drive/MyDrive/asl_model_saves/Vit_asl.pth\")\n"
      ],
      "metadata": {
        "id": "qWhaVu1XRCYs"
      },
      "execution_count": 5,
      "outputs": []
    }
  ]
}